{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadi Heidari Rad - 810197011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI - CA4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "data = pd.read_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, shuffling the data and then allocating 0.8 of it to train data and the rest 0.2 to test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling dataset:\n",
    "data = data.sample(frac = 1)\n",
    "\n",
    "separated_data = np.split(data, [round(len(data) * 0.8)], axis=0)\n",
    "train_data_x = separated_data[0].drop(columns=['target'])\n",
    "train_data_y = separated_data[0]['target']\n",
    "\n",
    "test_data_x = separated_data[1].drop(columns=['target'])\n",
    "test_data_y = separated_data[1]['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the train data to predict the test one with decision tree and then calcualting accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7377049180327869\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(train_data_x, train_data_y)\n",
    "result = clf.predict(test_data_x)\n",
    "\n",
    "print(\"accuracy: \" + str(accuracy_score(test_data_y, result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1 and 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make 5 trees with 150 samples of train data and then save the predicted list somewhere, at last for each test data element, final predicted value will be the value which is most predicted by those 5 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8032786885245902\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "final = []\n",
    "num_of_trees = 5\n",
    "for i in range (0, num_of_trees):\n",
    "    bagging_data = separated_data[0].sample(n = 150)\n",
    "    bagging_train_x = bagging_data.drop(columns=['target'])\n",
    "    bagging_train_y = bagging_data['target']\n",
    "    \n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf = clf.fit(bagging_train_x, bagging_train_y)\n",
    "    result = clf.predict(test_data_x)\n",
    "    res.append(result)\n",
    "\n",
    "\n",
    "for i in range (0, len(res[0])):\n",
    "    sum = 0\n",
    "    for j in range(0, len(res)):\n",
    "        sum += res[j][i]\n",
    "    if sum > (num_of_trees/2):\n",
    "        final.append(1)\n",
    "    else:\n",
    "        final.append(0)\n",
    "\n",
    "print(\"accuracy: \" + str(accuracy_score(test_data_y, final)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting columns one by one and then making the decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy without age: 0.7540983606557377\n",
      "accuracy without sex: 0.7213114754098361\n",
      "accuracy without cp: 0.639344262295082\n",
      "accuracy without trestbps: 0.6721311475409836\n",
      "accuracy without chol: 0.7049180327868853\n",
      "accuracy without fbs: 0.7213114754098361\n",
      "accuracy without restecg: 0.6721311475409836\n",
      "accuracy without thalach: 0.7049180327868853\n",
      "accuracy without exang: 0.7213114754098361\n",
      "accuracy without oldpeak: 0.7213114754098361\n",
      "accuracy without slope: 0.7704918032786885\n",
      "accuracy without ca: 0.639344262295082\n",
      "accuracy without thal: 0.7377049180327869\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for (columnName, columnData) in train_data_x.iteritems():\n",
    "    train_without_one_col = train_data_x.drop(columns=[columnName])\n",
    "    test_without_one_col = test_data_x.drop(columns=[columnName])\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf = clf.fit(train_without_one_col, train_data_y)\n",
    "    result = clf.predict(test_without_one_col)\n",
    "\n",
    "    print(\"accuracy without \"+ str(columnName)+ \": \"+ str(accuracy_score(test_data_y, result)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choosing 5 features among 13 (by random) and calculating the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7868852459016393\n"
     ]
    }
   ],
   "source": [
    "def split_five_features(train_x, train_y, test_x, test_y):\n",
    "\n",
    "    for i in range(0, 8):\n",
    "        rnd = random.randrange(0, len(train_x.columns))\n",
    "        train_x = train_x.drop(train_x.columns[rnd], axis = 1)\n",
    "        test_x = test_x.drop(test_x.columns[rnd], axis = 1)\n",
    "\n",
    "    # print(five_train)\n",
    "    # print(five_test)\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf = clf.fit(train_x, train_y)\n",
    "    result = clf.predict(test_x)\n",
    "    return result\n",
    "\n",
    "ans = accuracy_score(test_data_y, split_five_features(train_data_x, train_data_y, test_data_x, test_data_y))\n",
    "print(\"accuracy: \" + str(ans) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the goal is to implement bagging but with a difference: features are chosen randomized too (5 here) and this is called a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.819672131147541\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "final = []\n",
    "num_of_trees = 100\n",
    "for i in range (0, num_of_trees): # 100 trees\n",
    "    bagging_data = separated_data[0].sample(n = 150)\n",
    "    bagging_train_y = bagging_data['target']\n",
    "    bagging_train_x = bagging_data.drop(columns=['target'])\n",
    "    \n",
    "    result = split_five_features(bagging_train_x, bagging_train_y, test_data_x, test_data_y)\n",
    "    res.append(result)\n",
    "\n",
    "\n",
    "for i in range (0, len(res[0])):\n",
    "    sum = 0\n",
    "    for j in range(0, len(res) - 1):\n",
    "        sum += res[j][i]\n",
    "    if sum > (num_of_trees/2):\n",
    "        final.append(1)\n",
    "    else:\n",
    "        final.append(0)\n",
    "\n",
    "print(\"accuracy: \" + str(accuracy_score(test_data_y, final)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Boostrapping aggregation or bagging is when we choose to build more than one tree but with a fraction of sample numbers (compared to the original train data). these trees' train datas can have overlapping samples. Then each tree separately gives a list of prediction, and the final prediction for each test data sample is determined by the target that is most predicted. boostrapping also reduces the variance and standard deviation while retaining the bias because it frustrates effect of noise samples.\n",
    "<br><br>\n",
    "Overfitting is when our data analysis model is very fitted to the observed (or trained) data and cannot fit or predict new data well, and accurate. In a decision tree, over-fitting happens so likely because the tree is generated in a way to perfectly fit all samples in the training data set. So this effects the accuracy when predicting samples that are new. Bagging reduces overfitting. and the the reason is obvious, instead of fitting the model on just one sample of the train set, several models are fitted on different samples.\n",
    "<br><br>\n",
    "They are exactly the same except in one thing: In a random forest we do randomly select features too. But in Bagging all features are selected. So, a random forest is a bagging with also a feature bagging. Feature bagging (or Random Suspace Method) helps the model to escape overfitting on features, and we can combine fittings of several models and reduce the effect of some highly predictive or descriptive features overfitting.\n",
    "<br><br>\n",
    "There is an important point here!! <br>\n",
    "While the algorithms used here are very randomized, results are different each time running. And there were no information given about whether it is needed to consider one or more specific seed (random state) for decision trees in the project definition. So I didn't fix a random state. And results are different each time. \n",
    "<br>\n",
    "But clearly, 2.2 one works better on unfamiliar data than 1, and 2.5 one works better in all aspects(!), than part 2.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
